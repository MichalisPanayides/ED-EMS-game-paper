\subsection{Results}\label{sec:numerical_results}

This subsection aims to analyse how the gaming framework can affect the 
performance measures of the two hospitals and how to escape certain inefficient 
situations.

The most commonly used method for analysing normal form games is the Nash 
equilibrium described in section \ref{sec:methodology}.
Consider the following game:

\begin{multicols}{3}
    \begin{itemize}
        \item \( \lambda_{1_A} \) = 1
        \item \( \mu_A \) = 2
        \item \( C_A \) = 2
        \item \( N_A \) = 10
        \item \( M_A \) = 6
        \columnbreak

        \item \( \lambda_{1_B} \) = 2
        \item \( \mu_B \) = 2.5
        \item \( C_B \) = 2
        \item \( N_B \) = 10
        \item \( M_B \) = 6
        \columnbreak

        \item \( \lambda_2 \) = 2
        \item \( R \) = 2
        \item \( \alpha \) = 0.5
    \end{itemize}
\end{multicols}

The set of possible actions to choose from for player 1 and player 2 is the
set of thresholds that the EDs can choose from: 
\begin{equation}
    T_A \in [1, N_A], \quad T_B \in [1, N_B]
\end{equation}

For this example, the only Nash equilibrium of the game is achieved when both 
players choose a threshold of \( T_A = 10, T_B = 10 \).
This means that the two players' best response to each other is to only block
ambulances when they are full. 

Nash equilibria is a theoretical measure which can be
inconsistent with intuitive notions about what should be the outcome of a 
game \cite{myerson1978refinements}.
Therefore it might not be the best way to describe human behaviour.
Since the work of Maynard Smith \cite{smith1986evolutionary}, evolutionary game 
theory gives the tools for the emergence of stable behaviour.
One such model that allows for asymmetric payoffs, as is the case above, is 
replicator dynamics described in section 
(\ref{sec:methodology}).
Stable outcomes of this algorithm will correspond to a subset of Nash
equilibria but more importantly, will give a model of emergent behaviour. 

The use of a learning algorithm allows to investigate, not only the outcome of 
the game, but also how that outcome is reached.
Consider Figure \ref{fig:ard_by_itself}. 
By running asymmetric replicator dynamics on the system the behaviour that 
emerges can be observed.
It can be seen that for this particular set of parameters the strategies of 
the two hospitals converge over time. 
Both hospital 1 (row player) and hospital 2 (column player) seem to be playing 
the same strategy \(s_{10}\) which indicates that thresholds \(T_A = 10\) and 
\(T_B = 10\) are played.
What is more important in this example is how the two hospitals reached these
decisions which also highlights the importance of using a learning algorithm.
Hospital 2 is able to reach the decision in a short amount of time while 
hospital 1 takes longer and goes through numerous strategies to get there.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4, trim=120 0 120 0]{imgs/asymmetric_rd/asymmetric_rd.png}
    \caption{Asymmetric replicator dynamics run}
    \label{fig:ard_by_itself}
\end{figure}

In order to analyse how efficient the strategies played at every iteration are, 
the concept of the price of anarchy is used.
Price of anarchy is a measure that is used to quantify the efficiency of a
behaviour \cite{roughgarden2005selfish}.
In other words the price of anarchy is the worst-case equilibria measure and it 
is defined as:

\begin{equation}\label{eq:general_price_of_anarchy}
    \text{PoA} = \frac{\max_{s \in E} F(s)}{\min_{s \in S} F(s)}
\end{equation}

Here, \(S\) is the set of all sets of strategies \((s_A, s_B)\), \(E\) is the 
set of all possible sets of equilibria and \(F\) is the cost function to 
measure the efficiency for. 

To study the efficiency of the strategies being played, a new concept is 
introduced that considers the ratio between each hospital's best achievable 
blocking time and the one that is being played.
This new concept is defined as the compartmentalised price of anarchy of the 
players of the game and is defined as \(PoA_i(\tilde{s})\), where 
\(i \in \{A, B\}\) to distinguish among the two players/hospitals where 
\(\tilde{s}\) is the strategy that is being played by player \( i \). 
The compartmentalised price of anarchy aims to measure inefficiencies in the 
model.
The \(PoA\) for the blocking time of player \(i\) for strategy \(\tilde{s}\) 
is given by:

\begin{equation}\label{eq:poa_compartmentalised}
    PoA_{i}(\tilde{s}) = \frac{B_i(\tilde{s})}{\min_{s \in S_i} B_i(s)}
\end{equation}

% TODO: Make clear that we are speaking about a flooded model here.
Consider a game with two smaller (lower \( N_i, M_i \)) and busier (higher 
\(\lambda_{1_i} \) and \(\lambda_2\)) hospitals with the following set of 
parameters:

\begin{multicols}{3}
    \begin{itemize}        
        \item \( \lambda_{1_A} \) = 4.5
        \item \( \mu_A \) = 2
        \item \( C_A \) = 3
        \item \( T_A \in [1, N_A] \) 
        \item \( N_A \) = 6
        \item \( M_A \) = 5

        \columnbreak
        \item \( \lambda_{1_B} \) = 6
        \item \( \mu_B \) = 3
        \item \( C_B \) = 2
        \item \( T_B \in [1, N_B] \)
        \item \( N_B \) = 7
        \item \( M_B \) = 4
        
        \columnbreak
        \item \( \lambda_2 \) = 10.7
        \item \( R \) = 2
        \item \( \alpha \) = 0.9
    \end{itemize}
\end{multicols}

\textbf{Initial scenario:}
Using equation (\ref{eq:poa_compartmentalised}) and asymmetric replicator 
dynamics, the emergent behaviour can be measured and the compartmentalised price 
of anarchy at every iteration for both players.
Figure \ref{fig:ard_original} shows the strategies that are being played and 
the values of \(PoA_A(s)\) and \(PoA_B(s)\) for all iterations of the 
learning algorithm until it reaches an evolutionary stable pair of strategies.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{imgs/asymmetric_rd_and_PoA/asymmetric_original.pdf}
    \caption{The strategies played when running asymmetric replicator dynamics
    along with the compartmentalised price of anarchy of the blocking time at
    each iteration of the learning algorithm}
    \label{fig:ard_original}
\end{figure}

The learning algorithm reaches a stable pair of 
strategies where \(T_A = 5\) and \(T_B = 6\). After a number of iterations the
price of anarchy for both players stabilises and barely increases until the 
final iteration. 

\textbf{Increasing \(\lambda_2\):}
Figure \ref{fig:ard_lambda_2} shows a similar run of the
algorithm but when the strategies begin to stabilise an increase in the
arrival rate of ambulances occurs (i.e \( \lambda_2 = 24 \)).


\begin{figure}[H]
    \includegraphics[width=\textwidth]{imgs/asymmetric_rd_and_PoA/asymmetric_flooding.pdf}
    \caption{The strategies played when running asymmetric replicator dynamics
    along with the compartmentalised price of anarchy of the blocking time at
    each iteration of the learning algorithm. After a number of iterations the 
    arrival rate of ambulance patients is significantly increased to flood the
    system completely \( \lambda_2 = 24 \).}
    \label{fig:ard_lambda_2}
\end{figure}


By increasing \(\lambda_2\) there is no change as to how players behave
(\(T_A = 5, T_B = 6\)), but the efficiency of the system does change. 
There is a decline in the price of anarchy of the blocking time which at first 
glance indicates that upon flooding the system it becomes the loss in efficiency
due to rational individual behaviour decreases. 
This is non-sensical though.
What it really shows is that the steep increase in \( \lambda_2 \) leaves 
the system unable to cope regardless of the decisions made.

\textbf{Increasing number of servers \( C_A \) and \( C_B \):}
Figure \ref{fig:ard_num_of_servers} shows a run of asymmetric replicator 
dynamics with a change in the number of servers of the hospitals.
The number of servers are increased from \(C_A = 3, C_B = 2\) to 
\(C_A = 4, C_B = 3\).


\begin{figure}[H]
    \includegraphics[width=\textwidth]{imgs/asymmetric_rd_and_PoA/asymmetric_increase_C.pdf}
    \caption{
        The strategies played when running asymmetric replicator dynamics
        along with the compartmentalised price of anarchy of the blocking time 
        at each iteration of the learning algorithm. After a number of 
        iterations the number of servers for both systems are increased by one.
    }
    \label{fig:ard_num_of_servers}
\end{figure}

In this case, both the behaviour as well as the price of anarchy change.
The players change their strategies from \(T_A = 5, T_B = 6\) to 
\(T_A = 6, T_B = 7\) and the \(PoA\) of the blocking time goes down.
By adding more resources to the models they are able to increase their 
efficiency.
Although this is a good way to escape such inefficiencies, it might not always
be cost efficient.

\textbf{Incentivising players:}
From Figures \ref{fig:ard_lambda_2} and \ref{fig:ard_num_of_servers} it can be
seen that we can change some parameters of the model to make it more efficient.
The approach used on Figure \ref{fig:ard_penalty} is slightly different than 
the previous cases.
Once the played strategies in asymmetric replicator dynamics strategies 
converge the payoff matrices of the two are scaled in such a way so that the 
utilities of the selected strategy are penalised. This corresponds to a precise
policy change where more societally beneficial behaviours are incentivised.

Matrices \(A\) and \(B\) represent the original payoff matrices while matrices
\(\tilde{A}\) and \(\tilde{B}\) represent the incentivised payoff matrices.
It can be observed that matrix \(\tilde{A}\) is a scaled version of matrix 
\(A\) only on the row that is most frequently played and similarly matrix 
\(\tilde{B}\) of matrix \(B\) only on the column that is most frequently played
(matrix \(A\): row \(5\), matrix \(B\): column \(6\), see Figure 
\ref{fig:ard_penalty}).
Note that the values of the payoff matrices only differ on the fourth digit
which is the raw utility of the strategies played.

\tiny
\begin{equation*}
    A = 
    \begin{bmatrix}
        0.9995052 & 0.9995052 & 0.9995052 & 0.9995052 & 0.9995052 
        & 0.9995052 & 0.9995052 \\
        0.99954989 & 0.99954978 & 0.99954961 & 0.99954924 & 0.99954845 
        & 0.9995466 & 0.999539 \\
        0.99968233 & 0.99968194 & 0.99968151 & 0.99968067 & 0.99967874 
        & 0.9996734 & 0.999649  \\
        0.99990299 & 0.99990245 & 0.99990189 & 0.99990081 & 0.99989832
        & 0.9998909 & 0.9998517 \\
        \color{blue}0.99999996 & \color{blue}0.99999994 & \color{blue}0.99999992 
        & \color{blue}0.99999988 & \color{blue}0.99999973 & \color{blue}0.9999989 
        & \color{blue}0.9999859 \\
        0.9998773 & 0.99987995 & 0.99988236 & 0.99988643 & 0.99989417 
        & 0.9999126 & 0.9999712
    \end{bmatrix}
\end{equation*}

\begin{equation*}
    B = 
    \begin{bmatrix}
        0.99917127 & 0.99925823 & 0.99946188 & 0.99968499 & 0.9998942 
        & \color{red}1.0 & 0.99982128 \\
        0.99917127 & 0.99925479 & 0.99945638 & 0.99968051 & 0.99989153 
        & \color{red}0.99999997 & 0.9998333  \\
        0.99917127 & 0.99924532 & 0.99943793 & 0.99966451 & 0.99988286 
        & \color{red}0.99999966 & 0.99985269 \\
        0.99917127 & 0.99924146 & 0.99942878 & 0.99965484 & 0.99987667 
        & \color{red}0.99999921 & 0.99986701 \\
        0.99917127 & 0.9992342 &  0.99941013 & 0.99963286 & 0.99986077 
        & \color{red}0.9999972 & 0.9998958  \\
        0.99917127 & 0.99921279 & 0.99934961 & 0.99954933 & 0.99978407 
        & \color{red}0.99997106 & 0.99997276 \\
    \end{bmatrix}
\end{equation*}

\begin{equation*}
    \tilde{A} = 
    \begin{bmatrix}
        0.99950518 & 0.99950518 & 0.99950518 & 0.99950518 & 0.99950518
        & 0.99950518 & 0.99950518 \\
        0.99954989 & 0.99954978 & 0.99954961 & 0.99954924 & 0.99954845
        & 0.99954656 & 0.99953879 \\
        0.99968233 & 0.99968194 & 0.99968151 & 0.99968067 & 0.99967874
        & 0.99967339 & 0.9996492  \\
        0.99990299 & 0.99990245 & 0.99990189 & 0.99990081 & 0.99989832
        & 0.9998909 & 0.99985171 \\
        \color{blue}0.99969996 & \color{blue}0.99969994 & \color{blue}0.99969992 
        & \color{blue}0.99969988 & \color{blue}0.99969973 & \color{blue}0.99969895 
        & \color{blue}0.99968586 \\
        0.9998773 &  0.99987995 & 0.99988236 & 0.99988643 & 0.99989417
        & 0.99991264 & 0.99997122 \\
    \end{bmatrix}
\end{equation*}

\begin{equation*}
    \tilde{B} = 
    \begin{bmatrix}
        0.99917127 & 0.99925823 & 0.99946188 & 0.99968499 & 0.9998942
        & \color{red}0.9997 & 0.99982128 \\
        0.99917127 & 0.99925479 & 0.99945638 & 0.99968051 & 0.99989153
        & \color{red}0.99969997 & 0.9998333 \\
        0.99917127 & 0.99924532 & 0.99943793 & 0.99966451 & 0.99988286
        & \color{red}0.99969966 & 0.99985269 \\
        0.99917127 & 0.99924146 & 0.99942878 & 0.99965484 & 0.99987667
        & \color{red}0.99969921 & 0.99986701 \\
        0.99917127 & 0.9992342 &  0.99941013 & 0.99963286 & 0.99986077
        & \color{red}0.9996972 & 0.9998958 \\
        0.99917127 & 0.99921279 & 0.99934961 & 0.99954933 & 0.99978407
        & \color{red}0.99967106 & 0.99997276 \\
    \end{bmatrix}
\end{equation*}
\normalsize


\begin{figure}[H]
    \includegraphics[width=\textwidth]{imgs/asymmetric_rd_and_PoA/asymmetric_penalty.pdf}
    \caption{
        The strategies played when running asymmetric replicator dynamics
        along with the compartmentalised price of anarchy of the blocking time 
        at each iteration of the learning algorithm. After a number of 
        iterations the most dominant strategy is being penalised.
    }
    \label{fig:ard_penalty}
\end{figure}

Figure \ref{fig:ard_penalty} shows that players start playing strategies 
\( T_A = 5\) and \( T_B = 6 \) and mid-run of the learning algorithm a penalty
is applied to these strategies on the payoff matrix.
By incentivising the players in such a way the players change their strategies 
to \(T_A = 6\) and \(T_B = 7\), and thus ambulance patients are accepted in the 
ED more often.
Hence, the \(PoA\) for both EDs is decreased, meaning that the whole system is
more efficient in terms of the blocking time.
